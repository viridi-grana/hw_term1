{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть текст:\n",
    "\n",
    "In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a hobbit-hole, and that \n",
    "means comfort. \n",
    "It had a perfectly round door like a porthole, painted green, with a shiny yellow brass knob in the exact middle. The door opened on to a tube-shaped hall like a tunnel: a very comfortable tunnel without smoke, with panelled walls, and floors tiled and carpeted, provided with polished chairs, and lots and lots of pegs for hats and coats - the hobbit was fond of visitors. The tunnel wound on and on, going fairly but not quite straight into the side of the hill - The Hill, as all the people for many miles round called it - and many little round doors opened out of it, first on one side and then on another. \n",
    "No going upstairs for the hobbit: bedrooms, bathrooms, cellars, pantries (lots of these), wardrobes (he had whole \n",
    "rooms devoted to clothes), kitchens, dining-rooms, all were on the same floor, and indeed on the same passage. \n",
    "The best rooms were all on the lefthand side (going in), for these were the only ones to have windows, deep-set round windows looking over his garden, and meadows beyond, sloping down to the river. \n",
    "\n",
    "Соберите BPE-словарь размером не более 400 токенов для него. Можно написать собственный скрипт или использовать готовые, но если используете готовые, я попрошу прокомментировать их работу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# взято с https://github.com/DolbyUUU/byte_pair_encoding_BPE_subword_tokenization_implementation_python/blob/main/BPE.py\n",
    "\n",
    "# install and import libraries\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "class BPE():\n",
    "\n",
    "    def __init__(self, corpus, vocab_size):\n",
    "        \"\"\"Initialize BPE tokenizer.\"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "        \n",
    "    \n",
    "    # функция для предварительного разбиения текста на слова (использовала простой токенизатор, который был в одной из старых домашних задач)\n",
    "    def pre_tokenize(self, text):\n",
    "        processed_text = re.sub(r'[^-^\\w\\s]', '', text)\n",
    "        return re.sub(r'-(?!\\w)|(?<!\\w)-', '', processed_text).split()\n",
    "            \n",
    "    # тренируем токенизатор\n",
    "    def train(self):\n",
    "\n",
    "        # предварительно разбиваем текст на слова, считаем частотность для каждого слова текста, записываем в словарь\n",
    "        for text in self.corpus:\n",
    "            tokens = self.pre_tokenize(text)\n",
    "            for word in tokens:\n",
    "                self.word_freqs[word] += 1\n",
    "\n",
    "        # составляем базовый \"алфавит\" из всех символов корпуса (сначала в множестве, потом превращаем в список и сортируем)\n",
    "        alphabet = set()\n",
    "        for word in self.word_freqs.keys():\n",
    "            alphabet.update(list(word))\n",
    "        alphabet = sorted(alphabet)\n",
    "\n",
    "        # добавляем специальный токен </w> в начале \"алфавита\"\n",
    "        vocab = [\"</w>\"] + alphabet.copy()\n",
    "\n",
    "        # разбиваем каждое слово на отдельные символы, записываем в словарь\n",
    "        self.splits = {word: list(word) for word in self.word_freqs.keys()}\n",
    "\n",
    "        # объединяем самую частотную пару символов; шаг повторяется, пока не  будет достигнут необходимый объем словаря токенов (или пока pair_freqs не будет равен 0)\n",
    "        while len(vocab) < self.vocab_size:\n",
    "\n",
    "            # узнаем и записываем частотность каждой пары \n",
    "            pair_freqs = self.compute_pair_freqs()\n",
    "            # прерывается, если словарь pair_freqs пуст, потому что число запрошенных токенов превышает кол-во слов в тексте (больше нет вариантов для объединения)\n",
    "            if len(pair_freqs) == 0: \n",
    "                break\n",
    "            # находим самую частотную пару\n",
    "            best_pair = \"\"\n",
    "            max_freq = None\n",
    "            for pair, freq in pair_freqs.items():\n",
    "                if max_freq is None or max_freq < freq:\n",
    "                    best_pair = pair\n",
    "                    max_freq = freq\n",
    "\n",
    "            # объединяем самую частотную пару, добавляем в \"алфавит\"\n",
    "            self.splits = self.merge_pair(*best_pair)\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "            vocab.append(best_pair[0] + best_pair[1])\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        # считает частотность пар символов\n",
    "\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1: \n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "\n",
    "    def merge_pair(self, a, b):\n",
    "        # объединяет пару символов (в т.ч. символы, состоящие из объединенных ранее пар)\n",
    "\n",
    "        for word in self.word_freqs:\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    # перезаписываем в список символов, на которые разбито слово, объединенную пару с-в вместо этих с-в по отдельности\n",
    "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            self.splits[word] = split\n",
    "        return self.splits\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # токенизируем предложенный текст\n",
    "\n",
    "        tokenized_text = self.pre_tokenize(text)\n",
    "        splits_text = [list(word) for word in tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'n', 'a', 'hole', 'in', 'the', 'g', 'round', 'the', 'r', 'e', 'li', 'v', 'ed', 'a', 'hobbit', 'N', 'ot', 'a', 'n', 'as', 't', 'y', 'd', 'ir', 't', 'y', 'w', 'et', 'hole', 'f', 'i', 'l', 'led', 'with', 'the', 'e', 'nd', 's', 'of', 'w', 'or', 'm', 's', 'and', 'an', 'o', 'o', 'z', 'y', 's', 'me', 'll', 'n', 'or', 'y', 'et', 'a', 'd', 'r', 'y', 'b', 'ar', 'e', 's', 'and', 'y', 'hole', 'with', 'n', 'o', 'th', 'ing', 'in', 'it', 'to', 's', 'it', 'dow', 'n', 'on', 'or', 'to', 'e', 'a', 't', 'it', 'w', 'as', 'a', 'hobbit', '-', 'hole', 'and', 'th', 'a', 't', 'me', 'an', 's', 'c', 'om', 'for', 't', 'I', 't', 'ha', 'd', 'a', 'p', 'er', 'f', 'e', 'c', 't', 'ly', 'round', 'door', 'li', 'ke', 'a', 'p', 'or', 'th', 'o', 'le', 'p', 'a', 'in', 't', 'ed', 'g', 'r', 'e', 'e', 'n', 'with', 'a', 's', 'h', 'in', 'y', 'y', 'e', 'll', 'ow', 'b', 'r', 'as', 's', 'k', 'n', 'o', 'b', 'in', 'the', 'e', 'x', 'a', 'c', 't', 'm', 'i', 'd', 'd', 'le', 'The', 'door', 'o', 'pe', 'n', 'ed', 'on', 'to', 'a', 'tu', 'be', '-', 's', 'ha', 'p', 'ed', 'ha', 'll', 'li', 'ke', 'a', 'tunnel', 'a', 'ver', 'y', 'c', 'om', 'for', 't', 'a', 'b', 'le', 'tunnel', 'with', 'o', 'ut', 's', 'm', 'o', 'ke', 'with', 'p', 'an', 'el', 'led', 'w', 'all', 's', 'and', 'f', 'lo', 'or', 's', 't', 'i', 'led', 'and', 'c', 'ar', 'pe', 't', 'ed', 'p', 'ro', 'v', 'i', 'd', 'ed', 'with', 'p', 'o', 'li', 's', 'h', 'ed', 'c', 'ha', 'ir', 's', 'and', 'lots', 'and', 'lots', 'of', 'pe', 'g', 's', 'for', 'ha', 'ts', 'and', 'c', 'o', 'a', 'ts', 'the', 'hobbit', 'w', 'as', 'f', 'o', 'nd', 'of', 'v', 'i', 's', 'it', 'or', 's', 'The', 'tunnel', 'w', 'o', 'und', 'on', 'and', 'on', 'going', 'f', 'a', 'ir', 'ly', 'b', 'ut', 'n', 'ot', 'q', 'u', 'it', 'e', 'st', 'r', 'a', 'i', 'g', 'h', 't', 'in', 'to', 'the', 'side', 'of', 'the', 'h', 'i', 'll', 'The', 'H', 'i', 'll', 'as', 'all', 'the', 'pe', 'o', 'p', 'le', 'for', 'm', 'an', 'y', 'm', 'i', 'le', 's', 'round', 'c', 'a', 'l', 'led', 'it', 'and', 'm', 'an', 'y', 'l', 'it', 't', 'le', 'round', 'door', 's', 'o', 'pe', 'n', 'ed', 'o', 'ut', 'of', 'it', 'f', 'ir', 'st', 'on', 'on', 'e', 'side', 'and', 'the', 'n', 'on', 'an', 'o', 'the', 'r', 'N', 'o', 'going', 'u', 'p', 'st', 'a', 'ir', 's', 'for', 'the', 'hobbit', 'b', 'ed', 'rooms', 'b', 'a', 'th', 'rooms', 'c', 'e', 'll', 'ar', 's', 'p', 'an', 't', 'r', 'i', 'e', 's', 'lots', 'of', 'thes', 'e', 'w', 'ar', 'd', 'ro', 'be', 's', 'he', 'ha', 'd', 'w', 'hole', 'rooms', 'de', 'v', 'ot', 'ed', 'to', 'c', 'lo', 'thes', 'k', 'it', 'c', 'he', 'n', 's', 'd', 'in', 'ing', '-', 'rooms', 'all', 'were', 'on', 'the', 's', 'a', 'me', 'f', 'lo', 'or', 'and', 'i', 'nd', 'e', 'ed', 'on', 'the', 's', 'a', 'me', 'p', 'as', 's', 'a', 'g', 'e', 'The', 'be', 'st', 'rooms', 'were', 'all', 'on', 'the', 'le', 'f', 'th', 'and', 'side', 'going', 'in', 'for', 'thes', 'e', 'were', 'the', 'on', 'ly', 'on', 'e', 's', 'to', 'ha', 'v', 'e', 'wi', 'nd', 'ow', 's', 'de', 'e', 'p', '-', 's', 'et', 'round', 'wi', 'nd', 'ow', 's', 'lo', 'o', 'k', 'ing', 'o', 'ver', 'h', 'i', 's', 'g', 'ar', 'de', 'n', 'and', 'me', 'a', 'dow', 's', 'be', 'y', 'o', 'nd', 's', 'lo', 'p', 'ing', 'dow', 'n', 'to', 'the', 'r', 'i', 'ver']\n"
     ]
    }
   ],
   "source": [
    "text = ''' In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell,\n",
    "nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a hobbit-hole, and that means comfort.\n",
    "It had a perfectly round door like a porthole, painted green, with a shiny yellow brass knob in the exact middle.\n",
    "The door opened on to a tube-shaped hall like a tunnel: a very comfortable tunnel without smoke, with panelled walls,\n",
    "and floors tiled and carpeted, provided with polished chairs, and lots and lots of pegs for hats and coats - the hobbit was fond of visitors.\n",
    "The tunnel wound on and on, going fairly but not quite straight into the side of the hill - The Hill, as all the people for many miles round called it -\n",
    "and many little round doors opened out of it, first on one side and then on another.\n",
    "No going upstairs for the hobbit: bedrooms, bathrooms, cellars, pantries (lots of these), wardrobes (he had whole\n",
    "rooms devoted to clothes), kitchens, dining-rooms, all were on the same floor, and indeed on the same passage.\n",
    "The best rooms were all on the lefthand side (going in), for these were the only ones to have windows, deep-set round windows looking over his garden,\n",
    "and meadows beyond, sloping down to the river.'''\n",
    "\n",
    "bpe_tokenizer = BPE(corpus=[text], vocab_size=100)\n",
    "bpe_tokenizer.train()\n",
    "tokens = bpe_tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте расстояние Левенштейна для пар слов:\n",
    "\n",
    "программирование - лингвистика\n",
    "levenshtein - einstein\n",
    "Можно вручную, а можно с помощью скрипта, но в таком случае опять будьте готовы комментировать его работу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Левенштейна считала руками\n",
    "\n",
    "     п  р  о  г  р  а  м  м  и  р  о  в  а  н  и  е\n",
    "  0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16\n",
    "л 1  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16\n",
    "и 2  2  2  3  4  5  6  7  8  8  9  10 11 12 13 14 15\n",
    "н 3  3  3  3  4  5  6  7  8  9  9  10 11 12 12 13 14\n",
    "г 4  4  4  4  3  4  5  6  7  8  9  10 11 12 13 13 14\n",
    "в 5  5  5  5  4  4  5  6  7  8  9  10 10 11 12 13 14\n",
    "и 6  6  6  6  5  5  5  6  7  7  8  9  10 11 12 12 13\n",
    "с 7  7  7  7  6  6  6  6  7  8  8  9  10 11 12 13 13\n",
    "т 8  8  8  8  7  7  7  7  7  8  9  9  10 11 12 13 14\n",
    "и 9  9  9  9  8  8  8  8  8  7  8  9  10 11 12 12 13\n",
    "к 10 10 10 10 9  9  9  9  9  8  8  9  10 11 12 13 13\n",
    "а 11 11 11 11 10 10 9  10 10 9  9  9  10 10 11 12 13\n",
    "\n",
    "answer: 13\n",
    "\n",
    "     l  e  v  e  n  s  h  h  e  i  n\n",
    "  0  1  2  3  4  5  6  7  8  9  10 11\n",
    "e 1  1  1  2  3  4  5  6  7  8  9  10\n",
    "i 2  2  2  2  3  4  5  6  7  8  9  10\n",
    "n 3  3  3  3  3  3  4  5  6  7  8   9\n",
    "s 4  4  4  4  4  4  3  4  5  6  7   8\n",
    "t 5  5  5  5  5  5  4  4  4  5  6   7\n",
    "e 6  6  5  6  5  6  5  5  5  4  5   6\n",
    "i 7  7  6  6  6  6  6  6  6  5  4   5\n",
    "n 8  8  7  7  7  6  7  7  7  6  5   4\n",
    "\n",
    "answer: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите следующие регулярные выражения:\n",
    "\n",
    "Для поиска электронной почты. Считаем, что адрес может содержать только латинские буквы, цифры, нижнее подчеркивание и дефис (ну и собачку с точками).\n",
    "\n",
    "Для поиска пути файла в операционной системе UNIX или Windows (посмотрите: у них различающиеся стандарты). Регулярному выражению должно быть все равно, какая операционная система. Путь файла может быть абсолютным или относительным, может начинаться на букву диска. Регулярное выражение должно содержать две группы, в одной должно отлавливаться имя файла, а в другой его расширение. (Считаем, что у файла обязательно есть расширение).\n",
    "\n",
    "Дано предложение:\n",
    "\n",
    " ```\n",
    " #[[Time: Обычно\"обычно:#frequentative_adverbs_adj:FREQUENTATIVE\"] [Experiencer_Metaphoric: бюджет\"бюджет:бюджет:BUDGET\"] [[ко\"к:#preposition:PREPOSITION\"] [OrderInTimeAndSpace: второму\"второй:TWO_ORDINAL\"] Object_Situation: чтению \"чтение:READING_OF_THE_DRAFT_LAW\"] Predicate: готовится\"готовить:готовить:PREPAREDNESS\" [[DegreeApproximative: непосредственно\"непосредственный:DIRECT_OBLIQUE\"] [в\"в_Prepositional:#preposition:PREPOSITION\"] Locative: Думе\"дума:дума:DUMA\"]#: [[Agent: депутаты\"депутат:депутат:DEPUTY\"] Specification_Clause: корректируют\"корректировать:корректировать:TO_CORRECT\" [[Agent: правительственные\"правительство:правительство:GOVERNMENT\"] Object_Situation: планы\"план:план:SCHEDULE_FOR_ACTIVITY\"]]].\n",
    " ```\n",
    "В этом предложении каждое слово размечено семантической ролью, лексическим классом и семантическим классом. Роль - то, что идет сразу после квадратной скобки до :, потом идет само слово, потом его лемма, а через двоеточие идут лекс. класс и сем. класс (заглавными буквами). Напишите регулярное выражение, которое будет находить 1) сем. роль 2) словоформу 3) лемму 4) лексический класс 5) семантический класс в пяти группах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['veridi-grana@yandex.ru', 'viridi.grana@gmail.com']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for email\n",
    "\n",
    "import re\n",
    "\n",
    "email = 'veridi-grana@yandex.ru viridi.grana@gmail.com'\n",
    "re_email = r'([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)'\n",
    "pattern_email = re.compile(re_email)\n",
    "re.findall(pattern_email, email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Letter', 'txt')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for path (checking for UNIX)\n",
    "\n",
    "path = '/home/user/docs/Letter.txt'\n",
    "re_path = r'[\\/\\\\]([^\\/\\|:*?<>#%()][a-zA-Zа-яА-Я0-9]+).([a-z]+)$'\n",
    "pattern_path = re.compile(re_path)\n",
    "re.findall(pattern_path, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Letter', 'txt')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for path (checking for Windows)\n",
    "\n",
    "path = 'C:\\admin\\docs\\Letter.txt'\n",
    "re.findall(pattern_path, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time, Обычно, обычно, None, #frequentative_adverbs_adj:FREQUENTATIVE\n",
      "Experiencer_Metaphoric, бюджет, бюджет, бюджет, BUDGET\n",
      "OrderInTimeAndSpace, второму, второй, None, TWO_ORDINAL\n",
      "Object_Situation, чтению, чтение, None, READING_OF_THE_DRAFT_LAW\n",
      "Predicate, готовится, готовить, готовить, PREPAREDNESS\n",
      "DegreeApproximative, непосредственно, непосредственный, None, DIRECT_OBLIQUE\n",
      "Locative, Думе, дума, дума, DUMA\n",
      "Agent, депутаты, депутат, депутат, DEPUTY\n",
      "Specification_Clause, корректируют, корректировать, корректировать, TO_CORRECT\n",
      "Agent, правительственные, правительство, правительство, GOVERNMENT\n",
      "Object_Situation, планы, план, план, SCHEDULE_FOR_ACTIVITY\n"
     ]
    }
   ],
   "source": [
    "# for tagging\n",
    "\n",
    "# не вполне понятно, писать ли отдельно regex для предлогов: они отличаются по структуре. Также не совcем понимаю, почему у некоторых словоформ \n",
    "# как будто нет лексического класса, после словоформы и леммы сразу следует семантический класс.\n",
    "\n",
    "ling = '''[[Time: Обычно\"обычно:#frequentative_adverbs_adj:FREQUENTATIVE\"] [Experiencer_Metaphoric: бюджет\"бюджет:бюджет:BUDGET\"] \n",
    "[[ко\"к:#preposition:PREPOSITION\"] [OrderInTimeAndSpace: второму\"второй:TWO_ORDINAL\"] Object_Situation: чтению \"чтение:READING_OF_THE_DRAFT_LAW\"] \n",
    "Predicate: готовится\"готовить:готовить:PREPAREDNESS\" [[DegreeApproximative: непосредственно\"непосредственный:DIRECT_OBLIQUE\"] \n",
    "[в\"в_Prepositional:#preposition:PREPOSITION\"] Locative: Думе\"дума:дума:DUMA\"]#: [[Agent: депутаты\"депутат:депутат:DEPUTY\"] \n",
    "Specification_Clause: корректируют\"корректировать:корректировать:TO_CORRECT\" [[Agent: правительственные\"правительство:правительство:GOVERNMENT\"] \n",
    "Object_Situation: планы\"план:план:SCHEDULE_FOR_ACTIVITY\"]]].'''\n",
    "\n",
    "re_ling = r'([A-Z]{1}[A-Za-z_]+):.([А-Яа-я]+).?\\\"([а-я]+):([а-я]+)?:?([a-zA-Z_:#]+)\\\"'\n",
    "# pattern_ling = re.compile(re_ling)\n",
    "# re.findall(pattern_ling, ling)\n",
    "\n",
    "for elem in re.finditer(re_ling, ling):\n",
    "  print(elem.group(1), elem.group(2), elem.group(3), elem.group(4), elem.group(5), sep=', ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
